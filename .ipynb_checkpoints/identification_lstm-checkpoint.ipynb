{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    \n",
    "device = torch.device('cuda' if train_on_gpu else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"X\" (the neural network's training and testing inputs)\n",
    "\n",
    "def load_X(path):\n",
    "    X_signals = []\n",
    "    files = os.listdir(path)\n",
    "    files.sort(key=str.lower)\n",
    "    #file\n",
    "    #['train_acc_x.txt', 'train_acc_y.txt', 'train_acc_z.txt', 'train_gyr_x.txt', 'train_gyr_y.txt', 'train_gyr_z.txt']\n",
    "    for my_file in files:\n",
    "        fileName = os.path.join(path,my_file)\n",
    "        file = open(fileName, 'r')\n",
    "        X_signals.append(\n",
    "            [np.array(cell, dtype=np.float32) for cell in [\n",
    "                row.strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "        #X_signals = 6*totalStepNum*128\n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))#(totalStepNum*128*6)\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]],\n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    # Substract 1 to each output class for friendly 0-based indexing\n",
    "    y_ = y_ - 1\n",
    "    #one_hot\n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y_):\n",
    "    \"\"\"\n",
    "    Function to encode output labels from number indexes.\n",
    "    E.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \"\"\"\n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"..//test//train//x_train\"\n",
    "train_label_path = \"..//test//train//y_train.txt\"\n",
    "test_data_path = \"..//test//test//x_test\"\n",
    "test_label_path = \"..//test//test//y_test.txt\"\n",
    "\n",
    "train_x = load_X(train_data_path)\n",
    "train_y = load_y(train_label_path)\n",
    "test_x  = load_X(test_data_path)\n",
    "test_y  = load_y(test_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "test_data  = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "#dataloaders\n",
    "batch_size = 1500\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_hidden=64, n_layers=1, n_output=test_y.shape[1],\n",
    "                 drop_prob=0.5, lr=0.0025):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.lstm = nn.LSTM(6, n_hidden, n_layers,\n",
    "                           dropout=drop_prob, batch_first=True)\n",
    "        self.fc1 = nn.Linear(n_hidden,32)\n",
    "        self.fc2 = nn.Linear(32, n_output)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        r_output = self.dropout(r_output)\n",
    "        out = F.relu(self.fc1(hidden[-1]))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0025\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/tminhtet/mqp/env/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss 1.812719, Val Loss: 1.578055, Train Accuracy: 0.29, Val Accuracy: 0.63\n",
      "Epoch 2/30, Train Loss 0.988635, Val Loss: 0.994294, Train Accuracy: 0.59, Val Accuracy: 0.83\n",
      "Epoch 3/30, Train Loss 0.639580, Val Loss: 0.782793, Train Accuracy: 0.74, Val Accuracy: 0.86\n",
      "Epoch 4/30, Train Loss 0.524916, Val Loss: 0.621163, Train Accuracy: 0.82, Val Accuracy: 0.88\n",
      "Epoch 5/30, Train Loss 0.444105, Val Loss: 0.569189, Train Accuracy: 0.85, Val Accuracy: 0.89\n",
      "Epoch 6/30, Train Loss 0.393431, Val Loss: 0.544429, Train Accuracy: 0.87, Val Accuracy: 0.89\n",
      "Epoch 7/30, Train Loss 0.358443, Val Loss: 0.474239, Train Accuracy: 0.89, Val Accuracy: 0.91\n",
      "Epoch 8/30, Train Loss 0.290784, Val Loss: 0.525383, Train Accuracy: 0.89, Val Accuracy: 0.90\n",
      "Epoch 9/30, Train Loss 0.268499, Val Loss: 0.465212, Train Accuracy: 0.90, Val Accuracy: 0.90\n",
      "Epoch 10/30, Train Loss 0.295466, Val Loss: 0.470848, Train Accuracy: 0.91, Val Accuracy: 0.90\n",
      "Epoch 11/30, Train Loss 0.265433, Val Loss: 0.456066, Train Accuracy: 0.91, Val Accuracy: 0.90\n",
      "Epoch 12/30, Train Loss 0.249553, Val Loss: 0.449777, Train Accuracy: 0.92, Val Accuracy: 0.90\n",
      "Epoch 13/30, Train Loss 0.205102, Val Loss: 0.455460, Train Accuracy: 0.92, Val Accuracy: 0.91\n",
      "Epoch 14/30, Train Loss 0.214161, Val Loss: 0.417090, Train Accuracy: 0.93, Val Accuracy: 0.91\n",
      "Epoch 15/30, Train Loss 0.190563, Val Loss: 0.422366, Train Accuracy: 0.93, Val Accuracy: 0.92\n",
      "Epoch 16/30, Train Loss 0.211406, Val Loss: 0.390213, Train Accuracy: 0.93, Val Accuracy: 0.91\n",
      "Epoch 17/30, Train Loss 0.169607, Val Loss: 0.383107, Train Accuracy: 0.93, Val Accuracy: 0.91\n",
      "Epoch 18/30, Train Loss 0.169857, Val Loss: 0.425441, Train Accuracy: 0.94, Val Accuracy: 0.92\n",
      "Epoch 19/30, Train Loss 0.238710, Val Loss: 0.331565, Train Accuracy: 0.94, Val Accuracy: 0.93\n",
      "Epoch 20/30, Train Loss 0.151723, Val Loss: 0.401419, Train Accuracy: 0.94, Val Accuracy: 0.93\n",
      "Epoch 21/30, Train Loss 0.188079, Val Loss: 0.400673, Train Accuracy: 0.94, Val Accuracy: 0.93\n",
      "Epoch 22/30, Train Loss 0.173214, Val Loss: 0.406501, Train Accuracy: 0.94, Val Accuracy: 0.92\n",
      "Epoch 23/30, Train Loss 0.149878, Val Loss: 0.388911, Train Accuracy: 0.94, Val Accuracy: 0.93\n",
      "Epoch 24/30, Train Loss 0.169091, Val Loss: 0.453767, Train Accuracy: 0.95, Val Accuracy: 0.93\n",
      "Epoch 25/30, Train Loss 0.124400, Val Loss: 0.379058, Train Accuracy: 0.95, Val Accuracy: 0.93\n",
      "Epoch 26/30, Train Loss 0.109036, Val Loss: 0.372573, Train Accuracy: 0.95, Val Accuracy: 0.94\n",
      "Epoch 27/30, Train Loss 0.107099, Val Loss: 0.457036, Train Accuracy: 0.95, Val Accuracy: 0.93\n",
      "Epoch 28/30, Train Loss 0.114788, Val Loss: 0.423141, Train Accuracy: 0.95, Val Accuracy: 0.93\n",
      "Epoch 29/30, Train Loss 0.131486, Val Loss: 0.409102, Train Accuracy: 0.95, Val Accuracy: 0.93\n",
      "Epoch 30/30, Train Loss 0.138047, Val Loss: 0.424383, Train Accuracy: 0.95, Val Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "print_every = 30\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_accuracy, test_accuracy = 0, 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        #convert labels from one-hot to integer labels for crossEntropyLoss\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Reset backprop history from hidden\n",
    "        # Adjust the shape of hidden states for last batch\n",
    "        if i == len(train_loader) -1:\n",
    "            hidden = model.init_hidden(inputs.shape[0])\n",
    "        else:\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output, h = model(inputs, hidden)\n",
    "        #squeeze to remove single dimension entries ( [1, 1500, 20] -> [1500, 20])\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "#         top_p, top_class = output.topk(1, dim=1)\n",
    "#         output = torch.argmax(output, dim=1)\n",
    "        pred = torch.sigmoid(output).data > 0.5\n",
    "        pred = torch.argmax(output, dim=2)\n",
    "        equals = pred == labels.view(*pred.shape)\n",
    "#         equals = top_class == labels.view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.to(dtype=torch.float))\n",
    "        \n",
    "    else:\n",
    "        #validation loss\n",
    "        val_hidden = model.init_hidden(batch_size)\n",
    "        val_losses = []\n",
    "        model.eval()\n",
    "        for j, (inputs, labels) in enumerate(test_loader):\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            if j == len(test_loader) -1:\n",
    "                val_hidden = model.init_hidden(inputs.shape[0])\n",
    "            else:\n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "\n",
    "\n",
    "            output , val_hidden = model(inputs, val_hidden)\n",
    "            val_loss = criterion(output.squeeze(), labels)\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "#             top_p, top_class = output.topk(1, dim=1)\n",
    "#             equals = top_class == labels.view(*top_class.shape)\n",
    "            pred = torch.sigmoid(output).data > 0.5\n",
    "            pred = torch.argmax(output, dim=2)\n",
    "            equals = pred == labels.view(*pred.shape)\n",
    "            test_accuracy += torch.mean(equals.to(dtype=torch.float))\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        print('Epoch %d/%d, Train Loss %.6f, Val Loss: %.6f, Train Accuracy: %.2f, Val Accuracy: %.2f' %\n",
    "             (e+1, epochs, loss.item(), np.mean(val_losses),\n",
    "              train_accuracy/len(train_loader), test_accuracy/len(test_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'identify_lstm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load in the pretrained weights\n",
    "#state_dict = torch.load('identify_lstm.pth')\n",
    "#model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
