{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "# train_on_gpu = False\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    data = []\n",
    "    # 1.txt\n",
    "    file_names = os.listdir(data_path)\n",
    "    file_names.sort(key=lambda x:int(x[:-4]))\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(data_path, file_name)\n",
    "        signal_data = np.loadtxt(file_path)\n",
    "        data.append(signal_data)\n",
    "    data = np.array(data).transpose(0, 2, 1)\n",
    "    d_shape = data.shape\n",
    "    return data.reshape(d_shape[0], 1, d_shape[1], d_shape[2])\n",
    "\n",
    "def read_label(data_path):\n",
    "    data = []\n",
    "    # 1.txt\n",
    "    file_names = os.listdir(data_path)\n",
    "    file_names.sort(key=lambda x:int(x[:-10]))\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(data_path, file_name)\n",
    "        signal_data = np.loadtxt(file_path)\n",
    "        data.append(signal_data)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(519, 1, 6, 1024)\n",
      "(519, 1024)\n",
      "(58, 1, 6, 1024)\n",
      "(58, 1024)\n"
     ]
    }
   ],
   "source": [
    "train_data_path = \"..//data//train//train_data\"\n",
    "train_label_path = \"..//data//train//train_label\"\n",
    "test_data_path = \"..//data//test//test_data\"\n",
    "test_label_path = \"..//data//test//test_label\"\n",
    "\n",
    "train_data = read_data(train_data_path) # 519\n",
    "train_label = read_label(train_label_path)\n",
    "test_data = read_data(test_data_path) # 519\n",
    "test_label = read_label(test_label_path)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1_1): Conv2d(1, 64, kernel_size=(1, 17), stride=(1, 1), padding=(0, 8))\n",
      "  (conv1_2): Conv2d(64, 64, kernel_size=(1, 17), stride=(1, 1), padding=(0, 8))\n",
      "  (pool): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2_1): Conv2d(64, 128, kernel_size=(1, 17), stride=(1, 1), padding=(0, 8))\n",
      "  (conv2_2): Conv2d(128, 128, kernel_size=(1, 17), stride=(1, 1), padding=(0, 8))\n",
      "  (conv3_1): Conv2d(128, 256, kernel_size=(1, 17), stride=(1, 1), padding=(0, 8))\n",
      "  (conv3_2): Conv2d(256, 256, kernel_size=(1, 17), stride=(1, 1), padding=(0, 8))\n",
      "  (conv3_3): Conv2d(256, 256, kernel_size=(1, 17), stride=(1, 1), padding=(0, 8))\n",
      "  (up1): Upsample(size=(6, 512), mode=bilinear)\n",
      "  (conv3_4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv4_1): Conv2d(256, 128, kernel_size=(1, 17), stride=(1, 1), padding=(0, 8))\n",
      "  (conv4_2): Conv2d(128, 128, kernel_size=(1, 17), stride=(1, 1), padding=(0, 8))\n",
      "  (up2): Upsample(size=(6, 1024), mode=bilinear)\n",
      "  (upconv4_3): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv5_1): Conv2d(128, 64, kernel_size=(1, 17), stride=(1, 1), padding=(0, 8))\n",
      "  (conv5_2): Conv2d(64, 64, kernel_size=(6, 17), stride=(1, 1), padding=(0, 8))\n",
      "  (conv5_3): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(1, 64, (1,17), padding=(0,8))\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, (1,17), padding=(0,8))\n",
    "        \n",
    "        self.pool = nn.MaxPool2d((1,2), stride=(1,2))\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, (1,17), padding=(0,8))\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, (1,17), padding=(0,8))\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(128, 256, (1,17), padding=(0,8))\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, (1,17), padding=(0,8))\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, (1,17), padding=(0,8))\n",
    "        self.up1 = nn.Upsample(size=(6, 512), mode='bilinear', align_corners=True)\n",
    "        self.conv3_4 = nn.Conv2d(256, 128, (1,1))\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(256, 128, (1,17), padding=(0,8))\n",
    "        self.conv4_2 = nn.Conv2d(128, 128, (1,17), padding=(0,8))\n",
    "        self.up2 = nn.Upsample(size=(6, 1024), mode='bilinear', align_corners=True)\n",
    "        self.upconv4_3 = nn.Conv2d(128, 64, (1,1))\n",
    "        \n",
    "        self.conv5_1 = nn.Conv2d(128, 64, (1,17), padding=(0,8))\n",
    "        self.conv5_2 = nn.Conv2d(64, 64, (6, 17), padding=(0,8))\n",
    "        self.conv5_3 = nn.Conv2d(64, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        \n",
    "        x2 = self.pool(x)\n",
    "        \n",
    "        x2 = F.relu(self.conv2_1(x2))\n",
    "        x2 = F.relu(self.conv2_2(x2))\n",
    "        \n",
    "        x3 = self.pool(x2)\n",
    "        \n",
    "        x3 = F.relu(self.conv3_1(x3))\n",
    "        x3 = F.relu(self.conv3_2(x3))\n",
    "        x3 = F.relu(self.conv3_3(x3))\n",
    "        x3 = self.up1(x3)\n",
    "        x3 = self.conv3_4(x3)\n",
    "        \n",
    "        x4 = torch.cat([x3, x2], dim=1)\n",
    "        \n",
    "        x4 = F.relu(self.conv4_1(x4))\n",
    "        x4 = F.relu(self.conv4_2(x4))\n",
    "        x4 = self.up2(x4)\n",
    "        x4 = self.upconv4_3(x4)\n",
    "\n",
    "        x5 = torch.cat([x4, x], dim=1)\n",
    "        \n",
    "        x5 = F.relu(self.conv5_1(x5))\n",
    "        x5 = F.relu(self.conv5_2(x5))\n",
    "        x5 = self.conv5_3(x5)\n",
    "        \n",
    "        output = torch.sigmoid(x5)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "model = Net()\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.from_numpy(test_data).float()\n",
    "test_label = torch.from_numpy(test_label).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.from_numpy(train_data).float()\n",
    "train_label = torch.from_numpy(train_label).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss: 0.013153555305936662\n",
      "Epoch 10, Training loss: 0.013033241579068648\n",
      "Epoch 20, Training loss: 0.012921215768953739\n",
      "Epoch 30, Training loss: 0.01280708756003536\n",
      "Epoch 40, Training loss: 0.012701140461859215\n",
      "Epoch 50, Training loss: 0.012623115871568176\n",
      "Epoch 60, Training loss: 0.012552769657274662\n",
      "Epoch 70, Training loss: 0.012488389549227808\n",
      "Epoch 80, Training loss: 0.012428791536761158\n",
      "Epoch 90, Training loss: 0.012373087550863365\n",
      "Epoch 100, Training loss: 0.012320417228921072\n",
      "Epoch 110, Training loss: 0.012269986244295374\n",
      "Epoch 120, Training loss: 0.012221519169550181\n",
      "Epoch 130, Training loss: 0.012175012042527024\n",
      "Epoch 140, Training loss: 0.012130573650316006\n",
      "Epoch 150, Training loss: 0.01208808375002102\n",
      "Epoch 160, Training loss: 0.012046876153504917\n",
      "Epoch 170, Training loss: 0.012006579178721\n",
      "Epoch 180, Training loss: 0.011966929815867503\n",
      "Epoch 190, Training loss: 0.011927075870693993\n",
      "Epoch 200, Training loss: 0.011889799410093268\n",
      "Epoch 210, Training loss: 0.011853451788540291\n",
      "Epoch 220, Training loss: 0.011818231731251256\n",
      "Epoch 230, Training loss: 0.011784033824713014\n",
      "Epoch 240, Training loss: 0.01175087340075616\n",
      "Epoch 250, Training loss: 0.011718719049225883\n",
      "Epoch 260, Training loss: 0.011687268741672897\n",
      "Epoch 270, Training loss: 0.011656549050399104\n",
      "Epoch 280, Training loss: 0.011626153983362378\n",
      "Epoch 290, Training loss: 0.011596037616849176\n",
      "Epoch 300, Training loss: 0.011566545146738173\n",
      "Epoch 310, Training loss: 0.01153731673439114\n",
      "Epoch 320, Training loss: 0.011509118827781236\n",
      "Epoch 330, Training loss: 0.011481657566822104\n",
      "Epoch 340, Training loss: 0.011454618218317197\n",
      "Epoch 350, Training loss: 0.01142776213399707\n",
      "Epoch 360, Training loss: 0.011401202493779692\n",
      "Epoch 370, Training loss: 0.011374971597869961\n",
      "Epoch 380, Training loss: 0.011349214151185831\n",
      "Epoch 390, Training loss: 0.01132373088299654\n",
      "Epoch 400, Training loss: 0.011298288987780812\n",
      "Epoch 410, Training loss: 0.011273354693874\n",
      "Epoch 420, Training loss: 0.011248949233277456\n",
      "Epoch 430, Training loss: 0.01122494362050628\n",
      "Epoch 440, Training loss: 0.011201496097853189\n",
      "Epoch 450, Training loss: 0.011178499184591921\n",
      "Epoch 460, Training loss: 0.01115588505029219\n",
      "Epoch 470, Training loss: 0.011133620246297363\n",
      "Epoch 480, Training loss: 0.011111739341004507\n",
      "Epoch 490, Training loss: 0.011090210062920014\n",
      "Epoch 500, Training loss: 0.011068900038626383\n",
      "Time taken: 995.645441532135\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "import time\n",
    "t1 = time.time()\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    running_loss = 0\n",
    "\n",
    "    for i in range(0, len(train_data), batch_size):\n",
    "        data = train_data[i:i+batch_size]\n",
    "        label = train_label[i:i+batch_size]\n",
    "        if train_on_gpu:\n",
    "            data = data.cuda()\n",
    "            label = label.cuda()\n",
    "\n",
    "        log_ps = model(data)\n",
    "\n",
    "        loss = criterion(log_ps, label)\n",
    "        \n",
    "#         pred = log_ps >= 0.5\n",
    "#         correct = pred.eq(label).sum() / label.numel()\n",
    "#         correct = np.squeeze(correct.cpu().numpy())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    if e%10 == 0 or e==1:\n",
    "        print(\"Epoch {}, Training loss: {}\".format(e, running_loss/len(train_data)))\n",
    "        \n",
    "print(\"Time taken: {}\".format(time.time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = torch.from_numpy(np.array([0.5,0.4,0.6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.cuda()\n",
    "test_label = test_label.cuda()\n",
    "out = model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = out >= 0.5\n",
    "pred = pred.view(58, -1)\n",
    "correct = pred.eq(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58, 1024])\n",
      "torch.Size([58, 1024])\n",
      "torch.Size([58, 1024])\n",
      "tensor(50226, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "out.shape\n",
    "print(pred.shape)\n",
    "print(test_label.shape)\n",
    "print(correct.shape)\n",
    "\n",
    "print(correct.sum())\n",
    "print(correct.sum()/ (1024*58))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(traindata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
